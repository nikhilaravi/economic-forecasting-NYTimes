{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A script to scrape News Articles from The Guardian API\n",
    "\n",
    "Need to add in an API key to run the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def article_text(article):\n",
    "    soup = BeautifulSoup(article, \"lxml\")\n",
    "    return soup.get_text()\n",
    "\n",
    "apiKey = 'ADD_API_KEY_HERE'\n",
    "\n",
    "# format the url for the request\n",
    "def makeurl(options):\n",
    "    base = 'http://content.guardianapis.com/search?&api-key=' + apiKey + '&show-most-viewed=True&show-fields=headline%2Cbody&page-size=50';\n",
    "\n",
    "    if options['startDate']:\n",
    "        base = base + '&from-date=' + str(options['startDate'])\n",
    "    if options['endDate']:\n",
    "        base = base + '&to-date=' + str(options['endDate'])\n",
    "    if options['section']:\n",
    "        base = base + '&section=' + options['section']\n",
    "    if options.get('query'):\n",
    "        base = base + '&q=' + options['query']\n",
    "\n",
    "    return base;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests, json\n",
    "import dateutil.relativedelta as relativedelta\n",
    "import dateutil.rrule as rrule\n",
    "from dateutil.parser import *\n",
    "import datetime\n",
    "import os\n",
    "from helpers import article_text, makeurl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "before=datetime.datetime(2000,1,1) # start year 2000\n",
    "after=datetime.datetime(2016,10,31) # end year 2016 (october)\n",
    "\n",
    "findMondays = rrule.rrule(rrule.WEEKLY,byweekday=relativedelta.MO,dtstart=before) # function to find all the monday dates\n",
    "findSundays = rrule.rrule(rrule.WEEKLY,byweekday=relativedelta.SU,dtstart=before) # function to find all the sunday dates\n",
    "\n",
    "startDates =  [date.date() for date in findMondays.between(before,after,inc=True)] # find all the monday dates between the start and end year\n",
    "endDates =  [date.date() for date in findSundays.between(before,after,inc=True)] # find all the sunday dates between the start and end year\n",
    "\n",
    "# sections of the api which we want to query\n",
    "sections = ['business', 'politics', 'world', 'uk-news']\n",
    "\n",
    "columns=['Year', 'Week Start', 'Week End', 'Section', 'Number', 'Headline', 'Body Text']\n",
    "article_db = pd.DataFrame(columns=columns)\n",
    "\n",
    "# iterate through all the weeks\n",
    "for week in range(807,len(startDates)-1):\n",
    "\n",
    "    startDate = startDates[week]\n",
    "    endDate = endDates[week+1]\n",
    "\n",
    "    print 'Year: ', startDate.year, '   Week Beginning', startDate\n",
    "\n",
    "    # loop through the sections\n",
    "    for section in sections:\n",
    "\n",
    "        # create options object\n",
    "        options = {}\n",
    "        options['startDate'] = startDate\n",
    "        options['endDate'] = endDate\n",
    "        options['section'] = section\n",
    "        if section == 'world':\n",
    "            options['query'] = 'us%20OR%20uk%20OR%20eu'\n",
    "\n",
    "        # make the request after formatting the url with the correct query parameters\n",
    "        req = requests.get(makeurl(options))\n",
    "        # turn response into json\n",
    "        jtext = json.loads(req.text)\n",
    "        response = jtext.get('response')\n",
    "\n",
    "        # extract the array of articles\n",
    "        articles = response.get('results')\n",
    "\n",
    "        # print 'Num articles in section: ', section, response['pageSize']*response['pages']\n",
    "\n",
    "        # iterate through the first 30 articles\n",
    "        for num, article in enumerate(articles[0:30]):\n",
    "            # get the body text of the article\n",
    "            body = article['fields']['body']\n",
    "            headline = article['fields']['headline']\n",
    "            # convert the body html to a string\n",
    "            bodyHTML = str(body.encode(\"utf-8\"))\n",
    "            # extract text from html\n",
    "            bodyText = article_text(body).encode(\"utf-8\")\n",
    "            headlineText = article_text(headline).encode(\"utf-8\")\n",
    "\n",
    "            # create a row of data to append to DF\n",
    "            row = np.reshape([startDate.year, startDate, endDate, section, num, headlineText, bodyText], (1,7))\n",
    "\n",
    "            article_df = pd.DataFrame(row, columns=columns)\n",
    "            # article_db = article_db.append(article_df)\n",
    "\n",
    "            with open('./articles/articles_db.csv', 'a') as f:\n",
    "                article_df.to_csv(f, header=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
